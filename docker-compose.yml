services:
  # UI & Load Balancer
  dashboard:
    image: nginx:alpine
    container_name: app-dashboard
    ports:
      - "8081:80"
    volumes:
      - ./dashboard:/usr/share/nginx/html:ro
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - app-network
    depends_on:
      backend:
        # FIX: Changed from service_healthy to service_started 
        # because backend healthcheck is disabled.
        condition: service_started

  # Optimized Node.js Logic Tier
  backend:
    build:
      context: ./backend
      args:
        - DOCKER_GID=125
    image: db-autoscaler-backend:latest
    user: "1000:125"
    ports:
      - "5000:5000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      # FIX: Mount project dir so backend can read replica_count.txt written by autoscale.sh
      - .:/app/project:ro
    environment:
      - NODE_ENV=production
      - DB_HOST=primary-db
      - REPLICA_HOST=replica-db
      - DB_USER=root
      - DB_PASSWORD=production_secure_password
      - DB_NAME=app_db
      - NODE_OPTIONS=--max-old-space-size=400
    restart: always
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      disable: true # Keep disabled until the backend logic is 100% stable
    networks:
      - app-network
    depends_on:
      primary-db:
        condition: service_healthy # Reverted to healthy so backend doesn't crash if DB is booting

  # Dedicated Bash Scaler
  scaler:
    image: alpine:latest
    container_name: app-scaler
    # FIX: working_dir so 'docker compose' commands find docker-compose.yml
    working_dir: /app
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - .:/app
    environment:
      - COMPOSE_PROJECT_NAME=db-autoscaler
    # Critical: Use --remove-orphans in the scale-down logic inside autoscale.sh
    command: >
      sh -c "apk add --no-cache docker-cli docker-cli-compose bc mysql-client && 
             chmod +x /app/autoscale.sh && 
             sh /app/autoscale.sh"
    networks:
      - app-network
    depends_on:
      backend:
        condition: service_started

  # Data Tier - Primary
  primary-db:
    image: mysql:8.0
    container_name: primary-db
    environment:
      - MYSQL_ROOT_PASSWORD=production_secure_password
      - MYSQL_DATABASE=app_db
    healthcheck:
      test: [ "CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-pproduction_secure_password" ]
      interval: 5s
      timeout: 5s
      retries: 30
      # FIX: Increased to 120s because MySQL bootstrap is extremely slow here
      start_period: 120s
    networks:
      - app-network
    volumes:
      - mysql-primary-data:/var/lib/mysql
      # FIX: Mount primary.cnf so MySQL gets server-id and log-bin for replication
      - ./db-config/primary.cnf:/etc/mysql/conf.d/primary.cnf:ro
      - ./db-config/init-primary.sql:/docker-entrypoint-initdb.d/init.sql:ro

  # Data Tier - Replica
  replica-db:
    image: mysql:8.0
    environment:
      - MYSQL_ROOT_PASSWORD=production_secure_password
      - MYSQL_DATABASE=app_db
    networks:
      - app-network
    volumes:
      - mysql-replica-data:/var/lib/mysql
      # FIX: Mount replica.cnf so replicas load correct MySQL config (read_only, server-id)
      - ./db-config/replica.cnf:/etc/mysql/conf.d/replica.cnf:ro
      - ./db-config/init-primary.sql:/docker-entrypoint-initdb.d/init.sql:ro

  # Performance Testing (Containerized Locust)
  locust:
    image: locustio/locust:latest
    container_name: locust-load-test
    ports:
      - "8089:8089"
    volumes:
      - ./locust:/mnt/locust
    command: -f /mnt/locust/locustfile.py --host http://app-dashboard:80
    networks:
      - app-network
    depends_on:
      - backend

networks:
  app-network:
    driver: bridge

volumes:
  mysql-primary-data:
  mysql-replica-data:
